{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "This notebook contains solutions for the [week 1 assignments](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1. Knowing docker tags**\n",
    "\n",
    "Run the command to get information on Docker: `docker --help`. Now run the command to get help on the `docker build` command - which tag has the following text: *Write the image ID to the file*?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      --iidfile string          Write the image ID to the file\n"
     ]
    }
   ],
   "source": [
    "!docker build --help | grep \"Write the image ID to the file\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2. Understanding docker first run**\n",
    "\n",
    "Run docker with the *python:3.9* image in an interactive mode and the entrypoint of bash. Now check the python modules that are installed (use `pip list`). How many python packages/modules are installed?\n",
    "\n",
    "Answer (commands you need to run in order):\n",
    "\n",
    "    docker run -it --entrypoint=bash python:3.9\n",
    "\n",
    "    <After container has been run, you will be in able to execute commands inside the container using bash. Your bash prompt should look something like this: root@92ffd1826989:/#>\n",
    "    \n",
    "    pip list\n",
    "    Package    Version\n",
    "    ---------- -------\n",
    "    pip        22.0.4\n",
    "    setuptools 58.1.0\n",
    "    wheel      0.38.4\n",
    "    WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
    "    You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Database Setup**\n",
    "\n",
    "Before proceeding with the tasks, we need to do some preparation. \n",
    "\n",
    "Firstly, we need to run a local instance of PostgreSQL - I have created a basic setup using Docker Compose [here](docker-compose.yaml). This setup uses [pgAdmin](https://www.pgadmin.org/) for convinience reasons, although it is not necessary - we might as well interact with the database using other tools, like [pgcli](https://www.pgcli.com/) or simply *pandas* library.\n",
    "\n",
    "Secondly, we need to ingest some data to that database - the data we will be using is [NYC TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). Before May 2022, that data was stored in CSV files, which DataTalksClub backed up and made available [here](https://github.com/DataTalksClub/nyc-tlc-data). As I found some minor discrepancies between the backup CSV files and the \"current\" Parquet files, I would recommend using the former to make sure the output of your queries will be consistent with the answers to the questions.\n",
    "\n",
    "To ingest the data, we need to create tables with the appropriate schema inside our database. Although for this week we are only using Green Taxi Trip Records from Jan 2021 and Taxi Zone Lookup Table, we will create tables for each \"type\" of trips (except High Volume For-Hire Vehicle Trip Records, as the backup for these files is incomplete as of now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_data = pd.read_csv(\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2019-01.csv.gz\", nrows=100)\n",
    "green_data = pd.read_csv(\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-01.csv.gz\", nrows=100)\n",
    "fhv_data = pd.read_csv(\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-01.csv.gz\", nrows=100)\n",
    "\n",
    "# We are gonna load the entire Taxi Zone Lookup Table as we only have to ingest it once\n",
    "zone_data = pd.read_csv(\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some basic data cleanup, namely converting the appropriate columns in the datasets to *datetime* type (note, if you are working with Parquet files, you will not have this problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_data[\"tpep_pickup_datetime\"] = pd.to_datetime(yellow_data[\"tpep_pickup_datetime\"])\n",
    "yellow_data[\"tpep_dropoff_datetime\"] = pd.to_datetime(yellow_data[\"tpep_dropoff_datetime\"])\n",
    "\n",
    "green_data[\"lpep_pickup_datetime\"] = pd.to_datetime(green_data[\"lpep_pickup_datetime\"])\n",
    "green_data[\"lpep_dropoff_datetime\"] = pd.to_datetime(green_data[\"lpep_dropoff_datetime\"])\n",
    "\n",
    "fhv_data[\"pickup_datetime\"] = pd.to_datetime(fhv_data[\"pickup_datetime\"])\n",
    "fhv_data[\"dropOff_datetime\"] = pd.to_datetime(fhv_data[\"dropOff_datetime\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the schema from the `pandas.DataFrame` and generate the DDL statement compatibile with Postgres, we need an appropriate *SQLAlchemy Engine*. Note that the URL you will pass to the `create_engine` function depends on the configuration you have specified in the *docker-compose* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"postgresql://root:root@localhost:54320/nyc_tlc_trips\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we successfully established connection to the database, we can start creating tables and ingesting the data. For now, we are going to ingest the entire Taxi Zone Lookup data and only create tables for the remaining datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingesting Taxi Zone Lookup table\n",
    "zone_data.to_sql(\n",
    "    name=\"taxi_zone\", con=engine, if_exists=\"replace\", index=False\n",
    ")\n",
    "\n",
    "# Creating tables for Yellow Taxi Trips, Green Taxi Trips and For-Hire Vehicles Trips\n",
    "yellow_data.head(0).to_sql(\n",
    "    name=\"yellow_taxi_trip\", con=engine, if_exists=\"replace\", index=False\n",
    ")\n",
    "green_data.head(0).to_sql(\n",
    "    name=\"green_taxi_trip\", con=engine, if_exists=\"replace\", index=False\n",
    ")\n",
    "fhv_data.head(0).to_sql(name=\"fhv_trip\", con=engine, if_exists=\"replace\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see what kind of DDL was used to create the tables, you can see the exact queries through pgAdmin. You can also examine the data types assigned to your columns using pandas (note that this is not the **exact** query that was used to create the table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE taxi_zone (\n",
      "\t\"LocationID\" BIGINT, \n",
      "\t\"Borough\" TEXT, \n",
      "\t\"Zone\" TEXT, \n",
      "\tservice_zone TEXT\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.io.sql.get_schema(zone_data, con=engine, name=\"taxi_zone\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the trip data ingestion - as I will want to load more than a single month of data into the database -  I have wrote a [dedicated script](ingest_data.py). To see the arguments you can pass to the script you can run it like `python ingest_data.py --help`. As mentioned before, to complete this weeks assignments, we have to ingest Green Taxi Trip Records from Jan 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ingest_data.py green 2019 1 --db_name nyc_tlc_trips --db_port 54320"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note about data ingestion***\n",
    "\n",
    "In the *ingest_data.py* script I am using `pandas.DataFrame.to_sql` method to ingest data into the database. Although the volume of the data we are working with here is relatively small and this approach works just fine, sometimes - especially when working with larger volumes of data - one may want to try something more efficient. I would recommend researching different methods of data ingestion, like [here](https://ellisvalentiner.com/post/a-fast-method-to-insert-a-pandas-dataframe-into-postgres/) before undertaking such tasks.\n",
    "\n",
    "\n",
    "Now that the data has been ingested, we can proceed with the tasks. The remaining questions are supposed to be answered by running SQL queries against the data in our database (for example through pgAdmin)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3. Count records**\n",
    "\n",
    "How many taxi trips were made on January 15 in total (that is, started and finished on 2019-01-15)?\n",
    "\n",
    "Answer:\n",
    "\n",
    "    SELECT     COUNT(*)\n",
    "    FROM       green_taxi_trip\n",
    "    WHERE      CAST(lpep_pickup_datetime AS DATE) = '2019-01-15'\n",
    "               AND CAST(lpep_dropoff_datetime AS DATE) = '2019-01-15';\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4. Largest trip for each day**\n",
    "\n",
    "On which day the trip with the largest distance (use the pick up time for your calculations)?\n",
    "\n",
    "Answer:\n",
    "\n",
    "    SELECT     lpep_pickup_datetime\n",
    "    FROM       green_taxi_trip\n",
    "    ORDER BY   trip_distance DESC\n",
    "    LIMIT      1;\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5. The number of passengers**\n",
    "\n",
    "How many trips had 2 and 3 passengers on 2019-01-01?\n",
    "\n",
    "Answer:\n",
    "\n",
    "    SELECT     COUNT(CASE WHEN passenger_count = 2 THEN 1 END) AS n_two_passengers\n",
    "               , COUNT(CASE WHEN passenger_count = 3 THEN 1 END) AS n_three_passengers\n",
    "    FROM       green_taxi_trip\n",
    "    WHERE      CAST(lpep_pickup_datetime AS DATE) = '2019-01-01';"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6. Largest tip**\n",
    "\n",
    "For the passengers picked up in the Astoria Zone, which drop off zone had the largest tip (you should get the name of the zone, not the id)?\n",
    "\n",
    "Answer:\n",
    "\n",
    "\tSELECT     d_zone.\"Zone\" AS largest_tip_zone     \n",
    "\tFROM       green_taxi_trip AS trip\n",
    "\tINNER JOIN taxi_zone AS p_zone\n",
    "\tON         trip.\"PULocationID\" = p_zone.\"LocationID\"\n",
    "\tINNER JOIN taxi_zone AS d_zone\n",
    "\tON         trip.\"DOLocationID\" = d_zone.\"LocationID\"\n",
    "\tWHERE      p_zone.\"Zone\" = 'Astoria'\n",
    "\tORDER BY   trip.tip_amount DESC\n",
    "\tLIMIT      1;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Part B: Terraform\n",
    "\n",
    "The Terraform configuration can be found in the [terraform directory](./terraform/) and is mostly based on the [course default configuration](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/1_terraform_gcp/terraform). The notable differences between the two are:\n",
    "1. I have used [terraform.tfvars file](./terraform/terraform.tfvars) to define the variables. For the alternative ways of handling the input variables, please refer to [this article](https://developer.hashicorp.com/terraform/language/values/variables).\n",
    "2.  I have used the path to service account key file as authentication method instead of using the application default credentials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_airflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfbc7bbf1c7436e7ffe090bb5db41c3cddd847fd68379666776530bf84f447e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
