{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1. Execute spark.version - what's the output?**\n",
    "\n",
    "Answer:\n",
    "\n",
    "To do that, first we need to create `SparkSession` instance - we're going to run Spark locally with 3 worker threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/06 20:49:18 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.1.108 instead (on interface enp0s31f6)\n",
      "23/03/06 20:49:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/03/06 20:49:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf.setAppName(\"DEZoomCamp Week 5\")\n",
    "conf.setMaster(\"local[3]\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Spark version is accessible through `spark.version` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.1\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2. Read the High Volume FHV trips for June 2021, then repartition it to 12 partitions and save it in the `parquet` file format. What is the average size of the `parquet` files that were created (in MB)?**\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path(\"/home/michal/Projects/de_zoomcamp_23/data/raw/fhv_tripdata/fhvhv_tripdata_2021-06.csv.gz\")\n",
    "trips_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"dispatching_base_num\", T.StringType(), True),\n",
    "        T.StructField(\"pickup_datetime\", T.TimestampType(), True),\n",
    "        T.StructField(\"dropoff_datetime\", T.TimestampType(), True),\n",
    "        T.StructField(\"PULocationID\", T.IntegerType(), True),\n",
    "        T.StructField(\"DOLocationID\", T.IntegerType(), True),\n",
    "        T.StructField(\"SR_Flag\", T.StringType(), True),\n",
    "        T.StructField(\"Affiliated_base_number\", T.StringType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "trips = spark.read.csv(csv_path.as_posix(), header=True, schema=trips_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquet_path = Path(\"/home/michal/Projects/de_zoomcamp_23/data/transformed/fhv_tripdata/hvfhv_2021_06.parquet\")\n",
    "trips = trips.repartition(12)\n",
    "trips.write.parquet(parquet_path.as_posix(), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24M\t/home/michal/Projects/de_zoomcamp_23/data/transformed/fhv_tripdata/hvfhv_2021_06.parquet/part-00000-e72dbede-79dc-4560-b74e-d96b16484e46-c000.snappy.parquet\n",
      "24M\t/home/michal/Projects/de_zoomcamp_23/data/transformed/fhv_tripdata/hvfhv_2021_06.parquet/part-00001-e72dbede-79dc-4560-b74e-d96b16484e46-c000.snappy.parquet\n",
      "24M\t/home/michal/Projects/de_zoomcamp_23/data/transformed/fhv_tripdata/hvfhv_2021_06.parquet/part-00002-e72dbede-79dc-4560-b74e-d96b16484e46-c000.snappy.parquet\n",
      "24M\t/home/michal/Projects/de_zoomcamp_23/data/transformed/fhv_tripdata/hvfhv_2021_06.parquet/part-00003-e72dbede-79dc-4560-b74e-d96b16484e46-c000.snappy.parquet\n",
      "24M\t/home/michal/Projects/de_zoomcamp_23/data/transformed/fhv_tripdata/hvfhv_2021_06.parquet/part-00004-e72dbede-79dc-4560-b74e-d96b16484e46-c000.snappy.parquet\n",
      "24M\t/home/michal/Projects/de_zoomcamp_23/data/transformed/fhv_tripdata/hvfhv_2021_06.parquet/part-00005-e72dbede-79dc-4560-b74e-d96b16484e46-c000.snappy.parquet\n",
      "24M\t/home/michal/Projects/de_zoomcamp_23/data/transformed/fhv_tripdata/hvfhv_2021_06.parquet/part-00006-e72dbede-79dc-4560-b74e-d96b16484e46-c000.snappy.parquet\n",
      "24M\t/home/michal/Projects/de_zoomcamp_23/data/transformed/fhv_tripdata/hvfhv_2021_06.parquet/part-00007-e72dbede-79dc-4560-b74e-d96b16484e46-c000.snappy.parquet\n",
      "24M\t/home/michal/Projects/de_zoomcamp_23/data/transformed/fhv_tripdata/hvfhv_2021_06.parquet/part-00008-e72dbede-79dc-4560-b74e-d96b16484e46-c000.snappy.parquet\n",
      "24M\t/home/michal/Projects/de_zoomcamp_23/data/transformed/fhv_tripdata/hvfhv_2021_06.parquet/part-00009-e72dbede-79dc-4560-b74e-d96b16484e46-c000.snappy.parquet\n",
      "24M\t/home/michal/Projects/de_zoomcamp_23/data/transformed/fhv_tripdata/hvfhv_2021_06.parquet/part-00010-e72dbede-79dc-4560-b74e-d96b16484e46-c000.snappy.parquet\n",
      "24M\t/home/michal/Projects/de_zoomcamp_23/data/transformed/fhv_tripdata/hvfhv_2021_06.parquet/part-00011-e72dbede-79dc-4560-b74e-d96b16484e46-c000.snappy.parquet\n",
      "0\t/home/michal/Projects/de_zoomcamp_23/data/transformed/fhv_tripdata/hvfhv_2021_06.parquet/_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!du -sh /home/michal/Projects/de_zoomcamp_23/data/transformed/fhv_tripdata/hvfhv_2021_06.parquet/*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the size of each partition is around 24MB."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3. How many taxi trips were there on June 15? Consider only trips that started that day.**\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "452470"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips = spark.read.parquet(parquet_path.as_posix())\n",
    "\n",
    "trips_june_15 = trips.filter((F.col(\"pickup_datetime\") >= \"2021-06-15\") & (F.col(\"pickup_datetime\") < \"2021-06-16\"))\n",
    "trips_june_15.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4. How long was the longest trip in hours?**\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(pickup_datetime=datetime.datetime(2021, 6, 25, 13, 55, 41), dropoff_datetime=datetime.datetime(2021, 6, 28, 8, 48, 25), trip_duration_in_seconds=240764, trip_duration_in_hours=66.8788888888889)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.select(\"pickup_datetime\", \"dropoff_datetime\") \\\n",
    "    .withColumn(\"trip_duration_in_seconds\", F.col(\"dropoff_datetime\").cast(\"long\") - F.col(\"pickup_datetime\").cast(\"long\")) \\\n",
    "    .withColumn(\"trip_duration_in_hours\", F.col(\"trip_duration_in_seconds\") / 3600) \\\n",
    "    .orderBy(\"trip_duration_in_seconds\", ascending=False) \\\n",
    "    .head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5. Which port does the Spark's Web UI run on?**\n",
    "\n",
    "Answer:\n",
    "\n",
    "By default, every SparkContext launches a Web UI on port 4040."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6. Load the zone lookup data into a temporary view in Spark. Using this data, what is the name of the most frequent pickup location zone?**\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_path = Path(\"/home/michal/Projects/de_zoomcamp_23/data/raw/taxi_zone_lookup.csv\")\n",
    "zones = spark.read.csv(zones_path.as_posix(), header=True)\n",
    "\n",
    "trips.createOrReplaceTempView(\"Trips\")\n",
    "zones.createOrReplaceTempView(\"Zones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/06 20:50:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/06 20:50:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/06 20:50:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/06 20:50:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/06 20:50:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/06 20:50:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 13:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|                Zone|n_trips|\n",
      "+--------------------+-------+\n",
      "| Crown Heights North| 231279|\n",
      "|        East Village| 221244|\n",
      "|         JFK Airport| 188867|\n",
      "|      Bushwick South| 187929|\n",
      "|       East New York| 186780|\n",
      "|TriBeCa/Civic Center| 164344|\n",
      "|   LaGuardia Airport| 161596|\n",
      "|            Union Sq| 158937|\n",
      "|        West Village| 154698|\n",
      "|             Astoria| 152493|\n",
      "|     Lower East Side| 151020|\n",
      "|        East Chelsea| 147673|\n",
      "|Central Harlem North| 146402|\n",
      "|Williamsburg (Nor...| 143683|\n",
      "|          Park Slope| 143594|\n",
      "|  Stuyvesant Heights| 141427|\n",
      "|        Clinton East| 139611|\n",
      "|West Chelsea/Huds...| 139431|\n",
      "|             Bedford| 138428|\n",
      "|         Murray Hill| 137879|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT     z.Zone\n",
    "           , COUNT(*) AS n_trips\n",
    "FROM       Zones z\n",
    "LEFT JOIN  Trips t\n",
    "ON         z.LocationID = t.PULocationID\n",
    "GROUP BY   z.LocationID\n",
    "           , z.Zone\n",
    "ORDER BY   n_trips DESC;           \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_de",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17b6e26ac598d1945e73417e63c3dcb777c782c9a1511aa70092e7a5a3cb1848"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
